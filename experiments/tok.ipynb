{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf70a553",
   "metadata": {},
   "source": [
    "# Tokenizer experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4722734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports and global definitions loaded.\n",
      "Data directory: /Users/jg/code/cs336/gpt/data\n"
     ]
    }
   ],
   "source": [
    "from gpt.tokenizer import train, Tokenizer\n",
    "import json\n",
    "import base64\n",
    "from config import DATA_DIR, EOT\n",
    "\n",
    "\n",
    "print(\"Imports and global definitions loaded.\")\n",
    "print(f\"Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d433ae69",
   "metadata": {},
   "source": [
    "## TinyStories tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e12736b",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2856b880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer training complete.\n",
      "Out directory: /Users/jg/code/cs336/gpt/data/tinystories_bpe\n"
     ]
    }
   ],
   "source": [
    "def train_tinystories_tok():\n",
    "    vocab, merges = train(\n",
    "        DATA_DIR / \"TinyStoriesV2-GPT4-train.txt\",\n",
    "        10000,\n",
    "        [EOT],\n",
    "    )\n",
    "\n",
    "    out = DATA_DIR / \"tinystories_bpe\"\n",
    "    out.mkdir(exist_ok=True)\n",
    "\n",
    "    # ── vocab: id → bytes (stored as base64)\n",
    "    with open(out / \"vocab.json\", \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                str(i): base64.b64encode(tok).decode(\"ascii\")\n",
    "                for i, tok in vocab.items()\n",
    "            },\n",
    "            f,\n",
    "        )\n",
    "\n",
    "    # ── merges: one \"token1 token2\" per line, as base64\n",
    "    with open(out / \"merges.txt\", \"w\") as f:\n",
    "        for a, b in merges:\n",
    "            a_str = base64.b64encode(a).decode(\"ascii\")\n",
    "            b_str = base64.b64encode(b).decode(\"ascii\")\n",
    "            f.write(f\"{a_str} {b_str}\\n\")\n",
    "\n",
    "train_tinystories_tok()\n",
    "\n",
    "print(\"Tokenizer training complete.\")\n",
    "print(f\"Out directory: {DATA_DIR / 'tinystories_bpe'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55850bff",
   "metadata": {},
   "source": [
    "### Compression ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d40c77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing TinyStoriesV2-GPT4-train.txt:\n",
      "--------------------------------------------------------------------------------\n",
      "Document 1:\n",
      "Original text: Once upon a time, there was a little girl who loved eating her mother's cooking. Every day, the mom ...\n",
      "Original size: 900 bytes\n",
      "Number of tokens: 195\n",
      "Compression ratio: 4.62 bytes/token\n",
      "--------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "Original text: Once there was a small girl called Amy. She was three years old and loved to repeat the same things ...\n",
      "Original size: 725 bytes\n",
      "Number of tokens: 164\n",
      "Compression ratio: 4.42 bytes/token\n",
      "--------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "Original text: Once upon a time, there was a green ant named Andy. Andy loved to march all day long. One day, as An...\n",
      "Original size: 625 bytes\n",
      "Number of tokens: 148\n",
      "Compression ratio: 4.22 bytes/token\n",
      "--------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "Original text: Little Lucy was walking in the park one day when she saw something ugly. It was an opera singer! She...\n",
      "Original size: 970 bytes\n",
      "Number of tokens: 210\n",
      "Compression ratio: 4.62 bytes/token\n",
      "--------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "Original text: Once upon a time there was a boy called Dan. He was a lively and carefree child who loved to dance. ...\n",
      "Original size: 822 bytes\n",
      "Number of tokens: 202\n",
      "Compression ratio: 4.07 bytes/token\n",
      "--------------------------------------------------------------------------------\n",
      "Document 6:\n",
      "Original text: Once upon a time, there were two best friends. They were happy and they liked to play together. \n",
      "One...\n",
      "Original size: 553 bytes\n",
      "Number of tokens: 125\n",
      "Compression ratio: 4.42 bytes/token\n",
      "--------------------------------------------------------------------------------\n",
      "Document 7:\n",
      "Original text: Once upon a time, there was a little kitten. The kitten was very fit and loved to play. One day, it ...\n",
      "Original size: 718 bytes\n",
      "Number of tokens: 171\n",
      "Compression ratio: 4.20 bytes/token\n",
      "--------------------------------------------------------------------------------\n",
      "Document 8:\n",
      "Original text: Lily liked to write in her journal every day. She had a pink journal with a lock and a key. She wrot...\n",
      "Original size: 1998 bytes\n",
      "Number of tokens: 458\n",
      "Compression ratio: 4.36 bytes/token\n",
      "--------------------------------------------------------------------------------\n",
      "Document 9:\n",
      "Original text: One day a little boy wanted to go exploring. He was very curious and excited. He decided to climb th...\n",
      "Original size: 939 bytes\n",
      "Number of tokens: 218\n",
      "Compression ratio: 4.31 bytes/token\n",
      "--------------------------------------------------------------------------------\n",
      "Document 10:\n",
      "Original text: Once upon a time, there was a little boy named Tim. Tim was a careless boy who always made a mess. O...\n",
      "Original size: 690 bytes\n",
      "Number of tokens: 174\n",
      "Compression ratio: 3.97 bytes/token\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Overall statistics for TinyStoriesV2-GPT4-train.txt:\n",
      "Total bytes: 8940\n",
      "Total tokens: 2065\n",
      "Average compression ratio: 4.33 bytes/token\n",
      "\n",
      "Analyzing owt_valid.txt:\n",
      "--------------------------------------------------------------------------------\n",
      "Document 1:\n",
      "Original text: Here's a grerat collection of photos showing some of the costumes we'll be seeing in Zack Snyder's S...\n",
      "Original size: 751 bytes\n",
      "Number of tokens: 204\n",
      "Compression ratio: 3.68 bytes/token\n",
      "--------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "Original text: It all started with the simple idea of wanting to design something for myself. I wanted to create so...\n",
      "Original size: 4723 bytes\n",
      "Number of tokens: 1307\n",
      "Compression ratio: 3.61 bytes/token\n",
      "--------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "Original text: It doesn’t take an expert to know that Aaron Donald is a great football player. It’s an undeniable f...\n",
      "Original size: 1826 bytes\n",
      "Number of tokens: 582\n",
      "Compression ratio: 3.14 bytes/token\n",
      "--------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "Original text: Who Is Tom Cotton, The Man Behind The Iran Letter?\n",
      "\n",
      "Enlarge this image toggle caption Carolyn Kaster...\n",
      "Original size: 5032 bytes\n",
      "Number of tokens: 1533\n",
      "Compression ratio: 3.28 bytes/token\n",
      "--------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "Original text: Greetings Citizens,\n",
      "\n",
      "War may never change, but the tools sure do! We hope you’ve enjoyed testing som...\n",
      "Original size: 806 bytes\n",
      "Number of tokens: 236\n",
      "Compression ratio: 3.42 bytes/token\n",
      "--------------------------------------------------------------------------------\n",
      "Document 6:\n",
      "Original text: I was reconnecting with some of my former professors at a reception this past weekend when one of th...\n",
      "Original size: 15620 bytes\n",
      "Number of tokens: 4542\n",
      "Compression ratio: 3.44 bytes/token\n",
      "--------------------------------------------------------------------------------\n",
      "Document 7:\n",
      "Original text: Political workers from India's right-wing Maharashtra Navnirman Sena (MNS) party barged into an outl...\n",
      "Original size: 1666 bytes\n",
      "Number of tokens: 535\n",
      "Compression ratio: 3.11 bytes/token\n",
      "--------------------------------------------------------------------------------\n",
      "Document 8:\n",
      "Original text: The Hollywood Reporter and Spanish website El Pais have both reported that director of the upcoming ...\n",
      "Original size: 4101 bytes\n",
      "Number of tokens: 1184\n",
      "Compression ratio: 3.46 bytes/token\n",
      "--------------------------------------------------------------------------------\n",
      "Document 9:\n",
      "Original text: Falskaar Story, Dialogue, Quests, Scripting, World, Level and Dungeon Design by Alexander J. Velicky...\n",
      "Original size: 15883 bytes\n",
      "Number of tokens: 5344\n",
      "Compression ratio: 2.97 bytes/token\n",
      "--------------------------------------------------------------------------------\n",
      "Document 10:\n",
      "Original text: The Washington Post has published a “blind item” White House report — where not only its sources but...\n",
      "Original size: 1798 bytes\n",
      "Number of tokens: 527\n",
      "Compression ratio: 3.41 bytes/token\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Overall statistics for owt_valid.txt:\n",
      "Total bytes: 52206\n",
      "Total tokens: 15994\n",
      "Average compression ratio: 3.26 bytes/token\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "t = Tokenizer.from_files(DATA_DIR / \"tinystories_bpe\" / \"vocab.json\", DATA_DIR / \"tinystories_bpe\" / \"merges.txt\", [EOT])\n",
    "\n",
    "\n",
    "def analyze_compression(filename, split_by_eot=False):\n",
    "    # Read and sample 10 documents\n",
    "    with open(DATA_DIR / filename, \"r\") as f:\n",
    "        if split_by_eot:\n",
    "            # For TinyStories, split by EOT token\n",
    "            content = f.read()\n",
    "            documents = [doc.strip() for doc in content.split(EOT) if doc.strip()]\n",
    "        else:\n",
    "            # For OWT, split by newlines\n",
    "            documents = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "        # Sample 10 random documents\n",
    "        sampled_docs = random.sample(documents, 10)\n",
    "\n",
    "    # Encode documents and calculate stats\n",
    "    total_bytes = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    print(f\"\\nAnalyzing {filename}:\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for i, doc in enumerate(sampled_docs, 1):\n",
    "        # Get original size in bytes\n",
    "        doc_bytes = len(doc.encode(\"utf-8\"))\n",
    "        # Encode to tokens\n",
    "        tokens = t.encode(doc)\n",
    "        num_tokens = len(tokens)\n",
    "\n",
    "        total_bytes += doc_bytes\n",
    "        total_tokens += num_tokens\n",
    "\n",
    "        print(f\"Document {i}:\")\n",
    "        print(f\"Original text: {doc[:100]}...\")\n",
    "        print(f\"Original size: {doc_bytes} bytes\")\n",
    "        print(f\"Number of tokens: {num_tokens}\")\n",
    "        print(f\"Compression ratio: {doc_bytes/num_tokens:.2f} bytes/token\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    print(f\"\\nOverall statistics for {filename}:\")\n",
    "    print(f\"Total bytes: {total_bytes}\")\n",
    "    print(f\"Total tokens: {total_tokens}\")\n",
    "    print(f\"Average compression ratio: {total_bytes/total_tokens:.2f} bytes/token\")\n",
    "\n",
    "\n",
    "# Analyze both datasets\n",
    "analyze_compression(\"TinyStoriesV2-GPT4-train.txt\", split_by_eot=True)\n",
    "analyze_compression(\"owt_valid.txt\", split_by_eot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e85a41",
   "metadata": {},
   "source": [
    "### Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc3338cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting tokenization of datasets...\n",
      "\n",
      "Encoding /Users/jg/code/cs336/gpt/data/TinyStoriesV2-GPT4-train.txt to /Users/jg/code/cs336/gpt/data/TinyStoriesV2-GPT4-train_tokens.npy...\n",
      "Saved 540796778 tokens to /Users/jg/code/cs336/gpt/data/TinyStoriesV2-GPT4-train_tokens.npy.\n",
      "Original file size: 2124.55 MB\n",
      "Encoding time: 1332.68 seconds\n",
      "Throughput: 1.59 MB/s\n",
      "----------------------------------------\n",
      "\n",
      "Encoding /Users/jg/code/cs336/gpt/data/TinyStoriesV2-GPT4-valid.txt to /Users/jg/code/cs336/gpt/data/TinyStoriesV2-GPT4-valid_tokens.npy...\n",
      "Saved 5461210 tokens to /Users/jg/code/cs336/gpt/data/TinyStoriesV2-GPT4-valid_tokens.npy.\n",
      "Original file size: 21.46 MB\n",
      "Encoding time: 12.64 seconds\n",
      "Throughput: 1.70 MB/s\n",
      "----------------------------------------\n",
      "\n",
      "Dataset tokenization complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# t (tokenizer), DATA_DIR are assumed to be defined from previous cells.\n",
    "# If EOT was part of the tokenizer's special tokens, encode_iterable will handle it.\n",
    "\n",
    "datasets_to_encode = {\n",
    "    \"tinystories_train\": \"TinyStoriesV2-GPT4-train.txt\",\n",
    "    \"tinystories_valid\": \"TinyStoriesV2-GPT4-valid.txt\",\n",
    "}\n",
    "\n",
    "t = Tokenizer.from_files(DATA_DIR / \"tinystories_bpe\" / \"vocab.json\", DATA_DIR / \"tinystories_bpe\" / \"merges.txt\", [EOT])\n",
    "\n",
    "print(\"Starting tokenization of datasets...\")\n",
    "\n",
    "for name, filename in datasets_to_encode.items():\n",
    "    input_path = DATA_DIR / filename\n",
    "    # Ensure the output filename clearly indicates it contains tokens and the original dataset name\n",
    "    output_filename = f\"{filename.split('.')[0]}_tokens.npy\"  # e.g., TinyStoriesV2-GPT4-train_tokens.npy\n",
    "    output_path = DATA_DIR / output_filename\n",
    "\n",
    "    if not input_path.exists():\n",
    "        print(f\"Input file {input_path} not found. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nEncoding {input_path} to {output_path}...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    all_tokens = []\n",
    "    try:\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            # encode_iterable processes the file content iteratively (e.g., line by line)\n",
    "            # and yields token IDs. This is memory efficient.\n",
    "            for token_id in t.encode_iterable(f):\n",
    "                all_tokens.append(token_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding file {input_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    token_array = np.array(all_tokens, dtype=np.uint16)\n",
    "\n",
    "    try:\n",
    "        np.save(output_path, token_array)\n",
    "        print(f\"Saved {len(token_array)} tokens to {output_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file {output_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "\n",
    "    try:\n",
    "        file_size_bytes = os.path.getsize(input_path)\n",
    "        throughput_mb_per_second = (\n",
    "            (file_size_bytes / (1024 * 1024)) / duration\n",
    "            if duration > 0\n",
    "            else float(\"inf\")\n",
    "        )\n",
    "\n",
    "        print(f\"Original file size: {file_size_bytes / (1024**2):.2f} MB\")\n",
    "        print(f\"Encoding time: {duration:.2f} seconds\")\n",
    "        print(f\"Throughput: {throughput_mb_per_second:.2f} MB/s\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating throughput for {input_path}: {e}\")\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nDataset tokenization complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
